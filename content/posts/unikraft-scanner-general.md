---
title: "Unikraft-Scanner Overview"
summary: General Architecture and Information
date: 2025-07-26
series: ["PaperMod"]
weight: 1
aliases: ["/qq"]
tags: ["Unikraft-Scanner", "Unikraft"]
author: ["CSBonLaboratory"]
cover:
  #image: images/papermod-cover.png
  hiddenInList: true
---

> - What is static code analysis ?
> - Why is Unikraft "incompatible" with it ?
> - What are the solutions ?

---

### What is **static code analysis** ? ###

In order to find vulnerabilities, security solutions may employ various complex but  intelligent algorithms and heuristics that parse the source code of the targeted project and find possible vulnerabilites that further need human decisions regarding their impact and relevance.


To sum up, static analysis tools represent a first layer of automatic defense in catching the vast majority of issues and allows human specialists to dwelve deeper into fewer but complex vulnerabilities.

### Why is Unikraft "incompatible" with static code analysis ? ###

Static analyzers do not neceserarily read only the source code of the target project. Some do a little magic and somehow <span style="color: #4FFFA3;">"intercept" the compilation</span> process of the source files and extract various information that enhances the scanning process. For example, they may use the <span style="color: #4FFFA3;">abstract syntax tree</span> generated by the compiler instead of the raw source code.

This gives us a hint that the analysis process somehow depends on the compilation process. A simple experiment was presented [here](http://localhost:1313/posts/Unikraft-Scanner-Paper.pdf#section.3.2) that proves our hypothesis. <span style="color: #4FFFA3;">Only compiled code is analyzed not the entire source code !</span>



For the vast majority of software applications the entire code base finds its way, through compilation, in the executable binary. <span style="color: #4FFFA3;">However, Unikraft cannot achieve 100% scanning coverage since it is just a collection of modules where only a subset of them are plugged together with a specific user-level application.

An applications might need a certain combination of modules while another might need a totally diferent recipe of components.

A good representation can be viewed here:

<!-- ![alt text](/docs/entrypoints.png "Various configurations") -->

{{< figure src="/docs/entrypoints.png" alt="Macro modules" width="700" height="600" >}}

Let's suppose that we have 12 modules in Unikraft and 3 applications that may need 0, 1 or more (maybe all) modules provided by Unikraft to be bundled and linked into the final executable image.

Some application may share dependencies with others or may not. Static analysis on "Application 1" won't guarantee 100% scan coverage since only modules A,B,E,F are compiled thus scanned.


### Solution ###

1. We either invent a Unikraft application that requires all Unikraft modules (very big disadvantage since if we develop other Unikraft modules in the future we also need to change the application)

2. <span style="color: #4FFFA3;">The correct answer: Develop a platform that is based on a collaborative and iterative mechanism which allows developers to upload untested Unikraft recipes that may increase code coverage and enrich the already known vulnerabilites with new ones.</span>

For example, assuming we scanned in the past "Application 1", another user uploads a Unikraft build containg "Application 2". Now besides modules A, B, E, F that were scanned we enrich the scanned domain with modules I, J and D. Adjancency regarding modules found in the figure does not have any special meaning. <span style="color: #4FFFA3;"> You can imagine this process as a classic machine learning process where a new compilation recipe scheduled for analysis is a training epoch. </span>

### Another obstacle ? ###

Until now we encountered and solved problems at a macro level by trying and storing as many configurations/compile recipes as possible in order to enrich the scanning knowledge.

However, we still have issues if we zoom in on the meaning of a module. At the time of writing, the Unikraft codebase is largely developed in C. The modules that we talked about are just object files obtained by compiling 1 or more C source files.

Unfortunatelly, the same problem arises regarding scanning domain when we deal with C files due to conditional preprocess directives such as **#if, #ifdef, #ifndef, #elif, #else** which are intensively used by the kconfig mechanism of the Unikraft build system.

{{< figure src="/docs/compile_cov.png" alt="Micro source file" width="700" height="600" >}}

Compilation coverage problem can be viewed as multiple compilations that intertwine on the same codebase. Numerous scenarios are pictured above:
- source file can be compiled as a whole through only one configuration
- a file is partially compiled through one configuration while the rest through another
- both configurations can trigger common compilation background 
- none of the scenarios above since the source file is not part of an included module.


### Architecture ###

The general architecture is pictured below.

It resembles a pipeline in which a Unikraft-compatible high-level application is chosen together with its configuration (kcongig) to be scanned and potentially enrich the curent analyzed context/state with new code branches that have not been analyzed yet and subsequently its associated vulnerabilities.

{{< figure src="/docs/architecture_final.png" alt="Micro source file" width="800" height="700" >}}

The source code goes through 2 compilation steps:

**1. Discovery stage:** 
- use simplified compilation command generated by the kconfig (without any defined preprocessor symbol)
- detect postion of "compilation blocks" (regions of code between conditional preprocessor directives even nested regions) using a <span style="color: #4FFFA3;"> compiler plugin</span>
- start local static code analyzers and static code interceptor that builds an archive that will be analyzed in vendor's SaaS 

**2. Trigger stage:** 
- use the defined preprocessor symbols and same compilation commands from stage 1 and find all compiled code regions (compilation is triggered for blocks whose preprocessor condition has been satisfied by the newly introduced symbols)

Now that we have the results obtained by this application, we enrich the current context with potential newly compiled code regions.

For example, <span style="color: red;">red </span> regions were compiled in previous epochs and now we add new regions marked with <span style="color: blue;">blue </span>. This mechanism works as a feedback loop.



<!-- {{< pdfReader "/docs/Unikraft-Scanner-Paper.pdf" >}} -->



<!-- Static code analysis tools, such as Coverity or CodeQL, intercept the compilation stage of the target project, extract various information about the compiled code, build a database out of different compiler generated low-level representations and find possible vulnerabilities by querying the database for known weakness patterns. Thus, only compiled code will be analyzed.

However, Unikraft is made of multiple loosely-coupled C modules that can be chosen to be compiled, or not, depending on the top-level application's requirements, ported to run over the Unikraft unikernel. This means that the Unikraft code repo is not a single compilation target but multiple "mutations" that can have missing modules that are not critically required by the application. Such modules are represented by 1 or more C source files compiled into a single object file.

Unfortunately, besides entire C source files that may not be compiled in the current iteration, a C source file regarded by the build system to be compiled is not guaranteed to have been fully compiled. There may be statements such as #if, #elif, #else, #ifndef, #ifdef that rely on various configuration symbols that can deny their inner code to be compiled.

In order to fully scan the Unikraft codebase we need to trigger compilation of all core modules, in an incremental way (multiple retries where we configure differently the unikernel for novel "mutations"). Such practice is shown in the image below. -->